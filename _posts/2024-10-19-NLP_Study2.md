---
layout: single
title: "NLP 기초 스터디2"
categories: 스터디
tags: [NLP, 스터디]
redirect_from:
    - /스터디/NLP 기초 스터디2   
---
[Git](https://github.com/KyungbinKo?tab=repositories){: .btn .btn--danger}

참고자료: 밑바닥부터 시작하는 자연어 처리
# 신경망 복습
## 벡터와 행렬
### 벡터
* 크기와 방향을 가진 양(quantity) ↔ 스칼라(only 크기)
* 숫자가 일렬로 늘어진 집합. 1차원 배열(행렬은 2차원 배열)
* 벡터의 표현 방법은 행백터, 열벡터로 나뉨 <br>
![vec1]({{site.url}}/assets/images/자연언어처리/벡터1.png)

## 행렬 연소별 연산
### 원소별(element-wise) 연산
* 행렬(다차원 배열)에서 서로 대응하는 원소들끼리 연산<br>
![elementwise]({{site.url}}/assets/images/자연언어처리/element_wise.png){: .img-half}

## 브로드캐스트
* 형상이 다른 배열끼리 연산
* numpy 및 딥러닝 프레임워크에서 지원
![broadcast]({{site.url}}/assets/images/자연언어처리/broadcast.png)


## 벡터의 내적과 행렬의 곱
* 벡터의 내적: 두 벡터에서 대응하는 모든 원소들의 곱을 모두 더한 것<br>
$x=(x_1,...,x_n), y=(y_1,...,y_n)$ 에 대하여, $x \cdot y = x^Ty = x_1y_1 + x_2y_2 + ... + x_ny_n$
```python
a = np.array([1, 2, 3])
b = np.array([4, 6, 7])
np.dot(a, b)
# 32
```
* 행렬의 곱
<br>
![행렬곱]({{site.url}}/assets/images/자연언어처리/행렬곱.png){: .img-half}
```python
# 행렬의 곱
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
np.matmul(A, B)   # np.dot(A, B) 결과 같음
```
## 행렬 형상 확인
* 행렬과 벡터를 사용해 계산 시 형상(shape)에 주의해야 함
<br>
![형상확인]({{site.url}}/assets/images/자연언어처리/형상확인.png){: .img-half}
* A.shape, A.size() 등으로 확인 가능

## 신경망 추론
* 신경망: 단순한 함수로 간주할 수 있음 &#8594; 무엇인가를 입력하면, 무엇인가를 출력함
* 입력으로부터 출력을 만들어 내는 과정을 '추론(inference)'이라고 함
<br>
![신경망]({{site.url}}/assets/images/자연언어처리/신경망.png){: .img-half }
<br>
    * ○: 뉴런 또는 노드, &#8594;: 노드 사이의연결
    * &#8594;에는 가중치가 있으며, 가중치와 노드의 값을 곱해 다음 층 노드의 입력으로 씀
    * 각 층별로 정수 값이 있어 다음 층 노드 계산 시 가중치와 곱해 더해지며, 이를 편향(bias)이라고 부름
    * 한 층의 뉴런이 다음 층의 뉴런에 모두 연결된 경우 완전연결계층(fully-connected layer)이라 함

## 신경망 추론
![신경망 예시]({{site.url}}/assets/images/자연언어처리/신경망 예.png){: .img-half .align-center}
<br>
          신경망의 입력층-은닉층 관계를 수식으로 나타내면 다음과 같음<br>
              $h_1 = x_1w_{11}+x_2w_{21} + b$ <br>
$ (h_1,h_2,h_3,h_4) = (x_1, x_2) \begin{pmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\\ w_{21} & w_{22} & w_{23} & w_{24} \end{pmatrix} + (b_1, b_2, b_3, b_4)$ <br><br>
                &#8595; 벡터 연산으로 간소화 <br><br>
                 $h = xW + b$ <br>
![신경망 예시2]({{site.url}}/assets/images/자연언어처리/신경망 예2.png){: .img-half .align-center}

* 앞선 예시는 하나의 샘플 데이터(입력 데이터)를 가정했음
* 실제 신경망 추론 및 학습 과정에서는, 다수의 샘플 데이터를 한꺼번에 처리함: 미니배치(minibatch)
* 이 경우, x는 벡터 대신 행렬이 됨 <br>
    * 하나의 행이 하나의 샘플 <br>
![미니배치 형상확인]({{site.url}}/assets/images/자연언어처리/미니배치 형상확인.png){: .img-half }
<br>
* 실제 완전연결계층에서는 선형변환에 더해 활성화 함수가 적용: <br>
$\sigma (x) = \frac{1}{1+\exp(-x)}$ 
* 아래 예시는 시그모이드가 활성화 함수로 적용된 완전연결계층 신경망
<br>

```
x = np.random.randn(10, 2)
W1 = np.random.randn(2, 4)
b1 = np.random.randn(4)
W2 = np.random.randn(4, 3)
b2 = np.rando.randn(3)

h = np.matmul(x, W1) + b1
a = sigmoid(h)
s = np.matmul(a, W2) + b2
# 최종 출력의 형상은? (10, 3)
```

* 학습되지 않은 신경망은 '좋은 추론'을 할 수 없음
* 따라서, 먼저 학습을 한 후, 추론을 수행하는 것이 일반적
* 학습: 최적의 매개변수(가중치)를 찾는 과정

## 손실 함수
* 손실 함수(loss function): 학습이 얼마나 잘 되고 있는지를 알기 위한 '척도' <br>
    * 신경망 가중치에 따라 계산되는 출력과 정답을 입력으로 받아 손실에 관한 값(스칼라)을 반환
    * 신경망 모델의 종류에 따라, 손실 함수는 달라짐
    * 다중 클래스 분류의 경우 교차 엔트로피 오차(cross-entropy error)
![손실함수]({{site.url}}/assets/images/자연언어처리/손실함수.png){: .img-width-70 } <br>
* 소프트맥스 함수: 벡터를 입력받아 변환
    * 출력의 각 원소는 0 이상 1.0 이하의 실수이며, 모두 더하면 1이 됨 &#8594; 확률!!!
    * 신경망 출력으로부터 변환된 '확률'이 교차 엔트로피 오차의 입력이 됨 <br>
    $y_k = \frac{\exp(s_k)}{\sum_{i=1}^n \exp(s_i)}$
* 교차 엔트로피 오차
    * 정답 레이블은 t=[0,0,1]과 유사한 형태의 원핫 벡터로 표현
    * 이때, k번째 클래스가 정답인 경우 $t_k$는 1 <br>
    $L=-\sum_{k}t_k\log{y_k}$
* 미니 배치를 고려한 교차 엔트로피 오차
    샘플 1개 당 '평균' 손실 값을 의미
    $L=-\frac{1}{N}\sum_n\sum_kt_{nk}\log(y_{nk})$

## 미분과 기울기
* 스칼라 x, y에 대한 함수 y=f(x)가 있을 때, 미분 $\frac{dy}{dx}$의 의미는 x를 조금 변화시켰을 때 y가 얼마나 변화하는 가(변화율, 기울기)
* x가 벡터이고 L이 스칼라인 다변수 함수 L=f(x)의 경우, 미분값도 벡터의 형태를 띄며 이를 그래디언트(gradient)라 함: 벡터의 각 원소에 대한 미분을 배열 형태로 <br>
$\frac{\partial L}{\partial x}=(\frac{\partial L}{\partial x_1},\frac{\partial L}{\partial x_2},...,\frac{\partial L}{\partial x_n})$
* 가중치 행렬 W 기반 다변수 함수 L=f(w)의 경우, 그래디언트 값도 행렬 형태를 띔<br>
$\frac{\partial L}{\partial W}=\begin{pmatrix}
  \frac{\partial L}{\partial W_{11}} & \cdots & \frac{\partial L}{\partial W_{1n}} \\\ \vdots  & \ddots & \vdots \\\ \frac{\partial L}{\partial W_{m1}}  & \cdots & \frac{\partial L}{\partial W_{mn}}
 \end{pmatrix}$ <br>
 ★ W와 $\frac{\partial L}{\partial W}$는 형상이 같음
 
## 연쇄 법칙
 * 신경망 학습 시, 입력 데이터로부터 손실 값을 얻게 됨
 * 그 손실값으로부터 각 매개변수의 기울기를 계산해 매개변수를 갱신(학습)
 * 오차역전파(backprogapation): 매개변수의 기울기를 계산하기 위해 사용하는 방법
 * 연쇄 법칙(chain rule): 합성합수 미분에 대한 법칙
    * y=f(x)와 z=g(y)라는 두 함수가 있을 때, 이 합성함수의 미분은:
    $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$ <br>
    * 다루는 함수가 아무리 복잡하더라도, 개별 함수의 미분을 통해 합성함수의 미분 값을 계산할 수 있음

## 계산 그래프
* 계산 그래프: 노드와 화살표로 그려지며, 노드는 연산을 나타냄
* 화살표 방향대로 연산 처리 결과가 흐르며, 이를 순잔파(forward propatation)라 함
![계산그래프1]({{site.url}}/assets/images/자연언어처리/계산그래프1.png){: .img-half }
* 보다 일반적으로 나타내면, 노드 앞 뒤에도 임의의 계산 식이 있을 수 있음
* 노드는 복잡한 전체 계산의 일부를 나타내며, 신경망 학습의 경우 마지막에 손실값 계산
![계산그래프2]({{site.url}}/assets/images/자연언어처리/계산그래프2.png){: .img-half } 
* 신경망 학습 시 목표는 L의 미분값, 즉 각 매개변수에 대한 기울기를 구하는 것
* 계산 그래프의 역전파는 각 기울기를 계산하는 것 <br>
![계산그래프3]({{site.url}}/assets/images/자연언어처리/계산그래프3.png){: .img-half }
* 미분 값은 상류로부터 흘러온 미분과 각 연산 노드의 국소적인 미분을 곱해 계산
    * $\frac{\partial L}{\partial x}=\frac{\partial L}{\partial z} \frac{\partial z}{\partial x}$, $\frac{\partial L}{\partial y}=\frac{\partial L}{\partial z} \frac{\partial z}{\partial y}$ <br>

    ![계산그래프4]({{site.url}}/assets/images/자연언어처리/계산그래프4.png){: .img-half }
* 덧셈 노드의 역전파는 해석적으로 구할 수 있기 때문에, 다음과 같이 계산함
![계산그래프5]({{site.url}}/assets/images/자연언어처리/계산그래프5.png){: .img-half }
* 연산의 형태에 따라 미분값이 달라지기 때문에, 계산 그래프를 이용하여 빈번하게 나타나는 연산 별 미분값을 알아 둔다면, 연쇄 법칙과 함께 기울기를 계산할 수 있음
* 곱셈 노드<br>
![계산그래프6]({{site.url}}/assets/images/자연언어처리/계산그래프6.png){: .img-half }
* 분기 노드<br>
![계산그래프7]({{site.url}}/assets/images/자연언어처리/계산그래프7.png){: .img-half }
* Repeat 노드(복제)
    * 역전파: N개의 기울기를 더해서 계산<br>

    ![계산그래프8]({{site.url}}/assets/images/자연언어처리/계산그래프8.png){: .img-half }
* Sum 노드
    * 역전파: Repeat 순전파와 같음<br>

    ![계산그래프9]({{site.url}}/assets/images/자연언어처리/계산그래프9.png){: .img-half }
* MatMul 노드(행렬 곱셈) <br>
![계산그래프10]({{site.url}}/assets/images/자연언어처리/계산그래프10.png){: .img-half } <br>
    * 예시 연산 y = xW
    * x의 i 번째 원소 $x_i$에 대한 미분 <br>
    $\frac{\partial L}{\partial x_i}=\sum_j{\frac{\partial L}{\partial y_j}\frac{\partial y_j}{\partial x_i}}$ <br>
    * $x_i$를 변화시키면 y의 모든 원소가 변함 &#8594; 최종적으로 L이 변함
    * $x_i$에서 L에 이르는 경로는 다양하며, 그 총합이 기울기(gradient)
* 곱셈 노드 역전파 식을 참조하면, <br>
$\frac{\partial L}{\partial x_i}=\sum_j{\frac{\partial L}{\partial y_j}\frac{\partial y_j}{\partial x_i}}$ &#8594; $\frac{\partial L}{\partial x_i}=\sum_j{\frac{\partial L}{\partial y_j}W_{ij}}$
* $\frac{\partial L}{\partial x_i}$은 벡터 $\frac{\partial L}{\partial y}과 W의 i 행 벡터 내적
* 따라서, 다음 식 유도: 행렬의 곱으로 계산됨
$\frac{\partial L}{\partial x}=\frac{\partial L}{\partial y}W^T$ <br>
![계산그래프11]({{site.url}}/assets/images/자연언어처리/계산그래프11.png){: .img-half } <br>
* 미니배치를 고려한 MatMul 노드와 역전파
    * 매개변수의 기울기와 매개변수의 형상이 같음을 활용<br>
    ![계산그래프12]({{site.url}}/assets/images/자연언어처리/계산그래프12.png){: .img-70 } <br>
* 그 외 신경망 추론 과정에 많이 등장하는 연산과 역전파
![계산그래프13]({{site.url}}/assets/images/자연언어처리/계산그래프13.png){: .img-80 } <br>
<br>
![계산그래프14]({{site.url}}/assets/images/자연언어처리/계산그래프14.png){: .img-half } &#8594; MatMul, Repeat, +노드에 대한 역전파 참고해 계산<br>
* 신경망 추론 연산을 계산 그래프로 정의하면 합성 함수의 미분과 역전파를 이용해 그래디언트를 자동을 계산할 수 있음
    * 이 때, 그래디언트는 각 가중치가 손실에 미치는 영향을 나타냄
* 각 계산은 모듈화될 수 있음. 앞 연산과 뒤 연산에 관계 없이 계산 가능

## 가중치 갱신
* 신경망 학습 과정 <br>
1단계: 미니배치
    * 훈련 데이터 중에서 무작위로 다수의 데이터를 골라냄<br> 

    2단계: 기울기 계산
    * 오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기 구함<br>

    3단계: 매개변수 갱신 
    * 기울기를 사용하여 가중치 매개변수 갱신<br>

    4단계: 반복 
    * 1-3 단계 반복<br>
* 확률적 경사 하강법(stochastic gradient descent, SGD): 현재의 가중치를 기울기 반대 방향으로 일정한 거리만큼 갱신
    * $W ← W-\eta\frac{\partial L}{\partial W}$ <br>
    ![가중치 갱신1]({{site.url}}/assets/images/가중치 갱신1.png){: .img-half }
* SGD와 학습 의사 코드

```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
    
    def update(self, params, grads):
        for i in range(len(params)):
            params[i] -= self.lr * grads[i]

model = TwoLayerNet(...)
optimizer = SGE()

for i in range(10000):
    ...
    x_batch, t_batch = get_mini_batch(...) # 미니배치 획득
    loss = model.forward(x_batch, t_batch)
    model.backward()
    optimizer.update(model.params, model.grads)
    ...
```
* 파이토치 등 딥러닝 프레임워크에서도 SGD 등 옵티마이저 객체 선언, 모델 순전파, 역전파, 가중치 업데이트 순으로 학습 과정이 이루어짐