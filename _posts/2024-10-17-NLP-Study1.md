---
layout: single
title: "NLP 기초 스터디1"
categories: NLP
tags: [NLP, 스터디]
redirect_from:
    - /NLP/NLP 기초 스터디   
---
[Git](https://github.com/KyungbinKo?tab=repositories){: .btn .btn--danger}

# NLP Intro
## Natural Language Processing란?
자연어 처리(Natural Language Processing, NLP)는 컴퓨터에게 인간의 언어를 이해하고 상호작용 하도록 하는 기술에 관련된 학문입니다. 언어학, 컴퓨터 과학, 인공지능 등과 연관되어 있습니다.

## 언어의 용도
- 대화한다<br>
- 생각한다<br>
- 이야기를 전달한다<br>
- 공부한다<br>
- 사회적 관계를 만들고 유지한다<br>

## NLP 기술
### Applications(응용)
- 기계 번역(Machine translation)<br>
- 정보 검색(Information retrieval)<br>
- 질의 응답(Question answering)<br>
- 대화 시스템(Dialogue systems)<br>
- 정보 추출(Information extraction)<br>
- 문서 요약(Document summarization)<br>
- 감성 분석(Sentiment analysis)<br>
...

#### 기계 번역(Machine translation)
\- 하나의 언어로 작성된 텍스트를 다른 언어로 바꾸어 생성<br>
\- 사람 간 의사소통에 있어 핵심적인 역할을 함

#### 정보 검색(Information retrieval)
\- 큰 문서 집합으로부터 필요한 정보를 포함하는 자료를 찾는 시스템<br>
    - 예: 구글, 네이버 등 웹 검색 엔진<br>
\- 텍스트 뿐 아니라 이미지, 동영상 등을 다루는 검색 시스템으로 확장 가능함

#### 질의 응담(Question answering)
\- 사람의 질문에 답하는 시스템<br>
![QA]({{site.url}}/assets/images/QA_example.png)

#### 정보 추출(Information extraction)
\- 비정형 텍스트로부터 정보를 추출해 구조화<br>
![정보추출]({{site.url}}/assets/images/정보추출_example.png)

#### 감성 분석(Sentiment analysis)
\- 텍스트에 담긴 감성을 파악<br>
![감성분석]({{site.url}}/assets/images/감성분석_example.png)

#### 문서 요약(Document summarization)
![문서요약]({{site.url}}/assets/images/문서요약_example.png)

### Core technologies(핵심 기술)
- 언어 모델링(Language modeling)<br>
- 형태소 태깅(Part-of-speech tagging)<br>
- 구문 분석(Syntactic parsing)<br>
- 개체명 인식(Named-entity recognition)<br>
- 상호참조 해결(Coreference resolution)<br>
- 단어 의미 명확화(Word sense disambiguation)<br>
- 의미역 결정(Semantic role labelling)<br>
...

#### 언어 모델링(Language modeling)
\- Goal: compute the probability of a sentence or sequence of wordes<br>
  $P(W) = P(w_1, w_2, w_3, w_4, w_5, ..., w_n)$ <br>
<br>
\- Related task: probability of an upcoming word <br>
  $P(w_5|w_1,w_2,w_3,w_4)$ 
<br><br>
\* 이처럼 가능성을 계산하는 모델을 언어 모델이라고 부름

## 딥러닝 시대 이전의 언어 모델
       $P(w_1w_2...w_n)=\prod_i P(w_i|w_1w_2...w_{i-1})$ <br>
P("its water is so transparent") = P(its) x P(water|list) X P(is|its water) x P(so|its water is) x P(transparent|its water is so) <br>
      $P(w_i|w_1w_2...w_{i-1}) \approx P(w-i|w_{i-k}...w_{i-1})$ <br>
         $P(w_1w_2...w_n) \approx \prod_i P(w_i|w_{i-1})$ <br>
P("its water is so transparent") = P(its) x P(water|is) x P(is|its) x P(so|its) x P(transparent|its)

## 신경망이 언어 모델을 만나다
![Intro15pg]({{site.url}}/assets/images/Intro_15pg.png)

## LLM의 시대
![Intro16pg]({{site.url}}/assets/images/Intro_16pg.png)