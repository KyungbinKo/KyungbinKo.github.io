---
layout: single
title: "[논문 정리] Lost in the Middle: How Language Models Use Long Contexts"
categories: 논문
tags: [NLP, 논문 정리]
redirect_from:
    - /논문/Lost in the Middle
---
[Git](https://github.com/KyungbinKo?tab=repositories){: .btn .btn--danger}


# Lost in the Middle: How Language Models Use Long Contexts
Link: [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)

## Abstract

최근 언어 모델(LM)은 긴 컨텍스트를 입력으로 받을 수 있지만, 그것을 얼마나 잘 활용하는 지에 대해서는 잘 모름.

해당 논문에서는 입력 컨텍스트 내에 관련 정보를 식별해야 하는 2가지 task를 수행하여 성능을 분석함

- multi-document QA
- key-value retrieval

관련 정보의 위치에 따라 성능이 변함 → LM이 긴 입력 맥락에서 정보를 잘 활용하지 못함

- 관련 정보가 입력 컨텍스트의 시작 또는 끝에 위치하면 성능 Good
- **But**, 중간에 위치하는 경우 성능 Bad

### 논문에서 하고자 하는 일

LM이 입력 컨텍스트를 사용하는 지에 대한 더 나은 이해 제공

미래의 long-컨텍스트 LM을 위한 새로운 평가 프로토콜 제공

## Introduction

LM은 일반적으로 트랜스포머로 구현

- 입력 시퀀스의 길이에 따라 self-attention 복잡도가 제곱으로 증가함 → 긴 시퀀스 처리 불가

- 하드웨어 & 알고리즘 발전 → 긴 시퀀스 처리 가능

**But**, 입력 컨텍스트를 어떻게 활용하는지 불분명함

## 실험 1. multi-document QA

: 제공된 문서를 기반으로 추론하고 관련 정보를 찾아 질문에 답변하는 task

가설: 언어 모델이 긴 입력 컨텍스트 내에서 정보를 잘 활용할 수 있다면, 성능은 컨텍스트에서 관련 정보의 위치에 영향을 거의 받지 않을 것이다

변수: 입력 컨텍스트의 **크기**, 관련 정보의 **위치**

※ Experimental Setup

- input
    - 대답해야 할 질문
    - k 개의 문서(하나에만 질문에 대한 정보가 있음→k-1: distractor document)
        - NaturalQuestions-Open에서 query 가져와 사용

- 제공하는 문서의 수를 바꿔 입력 컨텍스트 크기 조절
    ![length]({{site.url}}/assets/images/Lost in the middle/length.png){: .img-width-half .align-center }

- 제공하는 문서의 순서를 바꿔 관련된 정보의 위치 조절
    ![position]({{site.url}}/assets/images/Lost in the middle/position.png){: .img-width-half .align-center }
    

※ Models

- Open Models
    - MPT-30B-Instruct → 8192 토큰
    - LongChat-13B(16K)
- Closed Models
    - GPT-3.5-Turbo → 4K 토큰
    - GPT-3.5-Turbo(16K)
    - Claude-1.3 → 8K 토큰
    - Claude-1.3(100K)

※ 모델 성능
![모델 성능]({{site.url}}/assets/images/Lost in the middle/모델 성능.png){: .img-width-70 .align-center }

- Closed-Book: 어떤 문서도 없이 매개변수 메모리에 의존해 정답 생성
- Oracle: 정답이 포함된 단일 문서를 제공
- open-book VS closed-book
    ![openVSclose]({{site.url}}/assets/images/Lost in the middle/openVSclose.png){: .img-width-half .align-center }
    
    - U자 모양의 성능 그래프(관련 정보가 처음과 끝에 있을 때 성능이 좋고, 중간에 있는 정보에 접근해야 하는 경우에 성능이 낮음) → 언어 모델이 긴 입력 컨텍스트에서 robust하게 정보에 접근하고 활용하지 못함
    - Closed-book에서 보다 정확도가 낮은 구간(Middle) 존재
- Results
    
    ![모델 비교]({{site.url}}/assets/images/Lost in the middle/모델 비교.png){: .img-width-70 .align-center }
    
    - 모델 성능은 관련 정보가 입력 컨텍스트의 시작이나 끝에 나타날 때 가장 높음
        - 현재 모델이 다운스트림 작업을 요청 받았을 때, 전체 컨텍스트 윈도우에 대해 효과적으로 추론할 수 없음
    - 확장 모델이라고 해서 입력 컨텍스트를 더 잘 활용하는 것은 아님

## 실험 2. key-value retrieval

: JSON 형식의 key-value 쌍을 제공하고, 그 중 특정 key에 대한 value를 답변하는 task

배경: 언어 모델이 multi-document QA 실험에서 입력 컨텍스트 중간에 정보를 검색하고 사용하는데 어려움을 겪는다는 점을 감안할 때, 어느 정도까지 입력 컨텍스트에서 간단히 검색할 수 있는가?

※ Experimental Setup

- input
    ![key_value]({{site.url}}/assets/images/Lost in the middle/key_value.png){: .img-width-70 .align-center }
    
1. key-value 쌍의 수를 조절해 입력 컨텍스트 크기 조절
2. 질문 key의 위치를 바꿔가며 실험 진행
- Results
    
    ![key_value_resutl]({{site.url}}/assets/images/Lost in the middle/key_value_result.png){: .img-width-70 .align-center }
    
    - 일부 모델은 task를 완벽히 수행하지만, 다른 모델은 입력 컨텍스트 중간에 위치하는 정보를 검색하는 데 어려움을 겪고, 마찬가지로 U자 모양의 성능 곡선을 나타냄

## 언어 모델이 관련 정보의 위치 변화에 강인하지 않은 이유

1. 모델 아키텍처의 효과
    - 평가한 open 모델은 모두 decoder-only 모델 → 각 타입 스텝에서 이전 토큰만 고려
    - encoder-decoder 모델(Flan-T5-XXL: 512 토큰, Flan-UL2: 2048 토큰) 실험
        
        ![아키텍쳐]({{site.url}}/assets/images/Lost in the middle/아키텍쳐.png){: .img-width-70 .align-center }
        
        - 인코더의 학습 시간 최대 시퀀스 길이보다 짧은 시퀀스에서 평가될 때는 관련 정보의 위치 변화에 Robust 함
        - **But**, 학습에서 본 것보다 긴 시퀀스에서 평가될 때는 U자 모양의 성능 곡선이 관찰됨
    
    ⇒ encoder-decoder 모델은 양방향 인코더를 통해 각 문서를 미래 문서의 컨텍스트에서 처리할 수 있으므로 컨텍스트 윈도우를 더 잘 활용할 수 있으며, 잠재적으로 문서 간의 상대적 중요도 추정 개선 가능
    
2. query-aware contextualization(위의 결과를 사용한 다른 접근)
    - 기존 실험은 처리할 데이터 뒤에 쿼리 배치 → decoder-only 모델은 문서 또는 key-value 쌍을 맥락화할 때 쿼리 토큰에 attend 불가
        - 쿼리는 프롬프트의 끝에만 나타나고 decoder-only 모델은 각 타입 스텝에서 이전 토큰에만 attend
    - 쿼리를 데이터 앞뒤에 배치해 쿼리 인식 맥락화를 활성화하여 decoder-only 모델을 개선할 수 있나?
        - encoder-decoder 모델(GPT-3.5-Turbo(16K))을 보면 쿼리 인식 맥락화가 키-값 검색에서 성능을 극적으로 개선함
        
        ![쿼리 맥락화]({{site.url}}/assets/images/Lost in the middle/쿼리 맥락화.png){: .img-width-half .align-center }
        
        - multi-document QA에서는 성능에 거의 영향 없음
            - 관련 정보가 맨 처음에 나타나면 성능이 약간 증가하지만, 그렇지 않으면 감소함
    - key-value 실험에 대해서는 거의 완벽, **But** multi-document QA에서는 Bad
3. instruction fine-tuning
    - MPT-30B-instruct VS MPT-30B(Base model)
        
        ![지시 파인 튜닝]({{site.url}}/assets/images/Lost in the middle/지시 파인 튜닝.png){: .img-width-half .align-center }
        
        - MPT-30B-instruct 모델의 성능이 베이스 모델보다 균일하게 높음, **But** 두 모델 모두 U자 모양 성능 곡선을 보임
        - 언어 모델이 명렁어 형식의 데이터로 프롬프트될 때 장거리 정보를 사용할 수 있음
    - 추가적인 fine-tuning과 모델 크기의 효과를 알아보기 위한 실험
        
        ![추가 실험]({{site.url}}/assets/images/Lost in the middle/추가 실험.png){: .img-width-half .align-center }
        
        - U자 모형 성능 곡선은 충분히 큰 언어 모델(13B, 70B)에서만 나타남 ↔ 7B 모델은 최근성 편향만 있음
        - Instruction fine-tuning과 인간 피드백 절차를 통한 강화 학습이 더 작은 모델(13B)에서 위치 편향을 완화하지만, 더 큰 모델(70B)에서는 영향이 거의 없음

## 더 많은 컨텍스트가 항상 좋은가?

- 입력 컨텍스트의 길이를 늘리는 것이 좋기만 하지는 않음(trade-off)
    
    → 더 많은 정보를 주는 것이지만, 동시에 모델이 추론해야 할 정보의 양도 늘어나는 것(accuracy ↓)
    
- 모델이 16,000개의 토큰을 처리할 수 있으면 16,000개의 토큰을 제공하는 것이 좋은가?
    
    → downstream task마다 다름 & 추가된 컨텍스트의 가치와 긴 입력 컨텍스트를 효과적으로 활용할 수 있는 지에 대한 모델의 능력에 따라 다름
    
- open-domain QA 환경(정답이 없거나 많을 수 있음)에서 retriever-reader 모델의 성능 평가
    - standard retriever-reader setup을 사용
    - retriever recall과 reader accuracy를 평가
    
    ![open_domain QA]({{site.url}}/assets/images/Lost in the middle/open_domain QA.png){: .img-width-50 .align-center }
    
    - Metric: Retrieval Recall
    - Retriever 성능이 포화되기 훨씬 전에 reader 모델 성능이 포화됨 → reader 모델이 추가 컨텍스트를 효과적으로 사용하지 않음
        
        → 모델이 input context의 시작 또는 끝에서 정보를 검색하고 사용하는데 더 능숙함
        
- 언어 모델 기반 reader가 검색된 컨텍스트를 사용하는 방식을 개선할 수 있는 방법론
    - **"Effective Reranking of Retrieved Documents"** <br>
        검색된 문서들을 다시 정렬하여 관련 정보를 입력 컨텍스트의 시작 부분에 더 가깝게 위치시키는 것
        
    - **"Ranked List Truncation"** <br>
        검색된 문서들의 리스트를 필요에 따라 줄이는 것
        
## 관련 연구

- Long-context language models
- How do language models use context?
- The serial-position effect

## 소감

이 연구는 LLM의 한계를 명확히 보여주며, 동시에 이를 개선할 수 있는 방향을 제시함. 특히 RAG 시스템과 같은 실제 응용 분야에서 이러한 발견이 중요한 의미를 가질 수 있음. 이러한 한계를 극복하고 더 효과적으로 긴 컨텍스트를 처리할 수 있는 방법을 모색해야 할 것.