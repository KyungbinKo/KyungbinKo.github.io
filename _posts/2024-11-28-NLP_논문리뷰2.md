---
layout: single
title: "[논문 정리] Which questions should I answer? Salience Prediction of Inquisitive Questions"
categories: 논문
tags: [NLP, 논문 정리]
redirect_from:
    - /논문/Salience Prediction of Inquisitive Questions
---
[Git](https://github.com/KyungbinKo?tab=repositories){: .btn .btn--danger}

# Title: Which questions should I answer? Salience Prediction of Inquisitive Questions
Link: [https://arxiv.org/pdf/2404.10917](https://arxiv.org/pdf/2404.10917)

## Abstract

Inquisitive question: 글을 읽을 때 발생하는 호기심에 의한 질문

최근 NLP 분야의 연구에서 LLM의 질문 생성 능력을 이용해 많이 발전을 이뤘지만, Inquisitive questions 분야는 미미함<br>
하나의 맥락에서 발생할 수 있는 질문은 무궁무진한데, 어떤 질문이 더 중요한 질문인가?

- QSALIENCE: Inquisitive questions의 중요성 예측 모델
    - 질문에 대한 답변이 글을 더 잘 이해할 수 있게 한다면 높은 점수 부여
    - 중요한 질문일수록 텍스트에서 답변될 가능성이 큼 → Questions Under Discussion(QUDs)
    - 중요한 질문에 답변하는 것이 뉴스 요약 퀄리티의 지표가 될 수 있음

## Instruction

질문을 하는 것은 궁금증에 대한 자연스러운 표현이고, ‘질문 생성’은 교육에 큰 영향을 미침

독자는 대화나 글을 읽는 과정에서 정보를 얻기 위해 질문을 함

NLP 분야에서 사전 학습 모델은 Inquisitive questions(open-ended, curiosity-driven, information-seeking)의 생성을 가능하게 했고, 이로 인해 최근 연구가 번창함:

- Identifying information loss between two texts
- Analyzing the diversity of news perspectives
- Generating elaborations or explanations
- Evaluating summaries
- Asking follow-up questions
- Decontextualization
- Planning

**But, Inquisitive Questions에 대한 연구는 아직 미미함**

하나의 맥락에서 여러 개의 질문이 발생할 수 있지만 모든 질문이 동등한 중요성을 갖지 않음

- Potential Question: 더 중요하다고 여겨질 수 있는 질문
    - QUD: Potential Questions 중 글의 뒷부분에서 답변되는 질문
    - 독자는 글이 어떻게 진행될 지 예측함 → QUDs의 예측 가능성에 대한 기반 제공
- 질문에 답변하는 것이 유용하다면, 그 질문은 중요한 질문

**But, 질문에 답변을 해야 하는지 또는 질문의 중요성을 예측하는 것에 대한 연구 없음**

- QSALIENCE: 맥락에서 중요성 점수는 예측하는 Instruction 튜닝 모델
![Figure1]({{site.url}}/assets/images/Inquisitive Questions/Figure 1.png){: .img-width-70 }
    
## Background

### 이론: Potential Questions and QUDs

1. Potential Question: 질문 Q의 답변 공간인 맥락 c에서 발화 u에 의해 유발된 질문 Q는 공통 영역 {c, u}에 속하고, c 하나로는 Q를 유발할 수 없음
2. 글은 이전 맥락에서 생성된 질문을 점진적으로 답변함
    - Q’이 발화 u’(공통 영역에 속하지 않지만, 그로 인해 발생함)에 의해 답변된다면, Q’은 QUD임
        - 문장 2에서 유발된 질문 Q1는 문장 3의 QUD
        - 문장 4에서 유발된 질문 Q3는 문장 5의 QUD
        - QUD는 훨씬 나중에 답변 될 수 있음
        
### 응용: Generating Inquisitive Questions

기존 연구는 질문의 중요도를 정의하지 않음 → 어떤 요소가 언어학적으로 적절한 QUDs를 생성하는 지에 초점을 둠
→ 어떤 개념에 대한 설명을 삽입할 지 예측할 수 없음

Goal-oriented 포럼에서, Rao and Daumé III가 순위 설명 질문에 대한 답변을 통해 정보 유용성 계산 <br>
→ 특정 과제 해결을 위한 명시적인 글을 전제로 함

## Task Defintion

독자는 문장{$1, …, S_{k-1}$}로 구성된 맥락 $C_p$로 구성된 글을 읽으면서 $S_k$(Anchor 문장)에 의한 potential question Q를 생성함

Q의 중요성: 문장 $S_k$에서 Q가 호출된 후 상황을 완전히 이해하기 위해 질문 Q에 답하는 것이 중요한 지에 대한 척도

- Score 0
    - Q에 문법적 오류가 있음
    - Q가 Anchor 문장에 의한 문장이 아님
    - Q가 다수의 하위 질문을 포함
    - 문맥을 잘못 해석함
- Score 1: Q는 $C_p$와 관련 없음
- Score 2: Q는 $C_p$와 관련은 있지만, 물어보는 것 자체가 불필요한 질문
- Score 3: Q는 $C_p$와 관련은 있지만, 답변을 하지 않아도 상관 없는 질문
- Score 4: Q는 $C_p$와 관련 있고, 답변하는 것이 새로 언급한 개념을 명확이 하거나, 전에 언급된 생각을 확장시킬 수 있음
- Score 5: Q는 $C_p$와 관련 있고, **반드시** 답변해야 함
    - Anchor 문장에서 소개된 개념을 명확히 함
    - 놀랍거나 미스테리한 일/물체에 대한 질문
    - 새로 소개된 인물에 대한 정보 요청
    - 질문에 대한 답변이 담화 이해에 필수

※ 중요성은 주관적일 수 있음 → 저자가 의도한 청중과 실제 독자의 배경 지식 간의 차이가 없다는 전제 하에 진행

## Data Collection

QSALIENCE 데이터

- Human Work + LLM Work
    - 영문 뉴스와 TED 강연에서 1766개의 Inquisitive question 질문에 중요성 + 평가에 대한 이유 주석 작업
- QSALINECE 데이터에 사용된 원천 기사/질문
    
    ![Table1]({{site.url}}/assets/images/Inquisitive Questions/Table1.png){: .img-width-40 }
    
    - DCQA 뉴스: $C_p$를 점진적으로 증가시키며 질문 생성
    - TED-Q의 TED 강연
    - DiverseSumm
        - DIV. Article
            - LLM이 생성한 질문에 중요성 주석을 단 뉴스 기사의 새로운 셋
            - 공정한 평가를 위해 분량 1500 단어로 제한
        - Div. TL;DR
            - GPT-4가 생성한 TL;DR에 중요성 주석을 담
- Machine Generated Questions: 독자가 궁금해 할 수 있는 문장의 일부에 대해 질문 5 개 생성
- 중요성 주석에 대한 IAA(주석자 간 일치) → Krippendorff’s α
    - DCQA: 0.719
    - TED-Q: 0.632
    - Div. Article: 0.751
    - Div. TL;DR: 0.649
- Analysis
    - 주석 달린 데이터 예시 <br>
        ![Data]({{site.url}}/assets/images/Inquisitive Questions/Data.png){: .img-width-40 }
        
    - QSALIENCE 데이터의 레이블 분포<br>
        ![Label]({{site.url}}/assets/images/Inquisitive Questions/Label.png){: .img-width-half }
        
        - LLM이 생성한 질문의 90.8% 이상이 유효함 → LLM은 Inquisitive question 생성에 타당함
        - 질문의 중요성 값이 다양함 → LLM 생성 질문에 대한 중요성 예측 모델 유용함            
        - 중요성 점수가 높은 질문 유형
            - Consequence
            - Example
            - Procedural
            - Disjuntive
            - Concept
            - Judgmental
        
## Salience VS. Answerability

$S_k$에 의한 유효한 potential question은 이후 글에서 답변된다면 $S_k$에 앵커된 QUD로 간주됨

Potential question과 QUD의 관계 →  답변 가능성(이후의 맥락 $C_s$에서 Q가 답변될 수 있는가)
![Answerability]({{site.url}}/assets/images/Inquisitive Questions/Answerability.png){: .img-width-half }
- Score 3: 완전히 답변됨
- Score 2: 일부만 답변됨
- Score 1: C_s에 의해 답변되지 않음
- Score 0: C_p + S_k에서 이미 답변됨

### Do salience and answerability correlate?

![Correlation]({{site.url}}/assets/images/Inquisitive Questions/Correlation.png){: .img-width-half }
- 중요한 potential questinos는 이후 글에서 답변될 가능성이 있음
- 독자와 작가의 예측이 어느 정도 일치함

### Are QUDs salient potential questions?

![QUD]({{site.url}}/assets/images/Inquisitive Questions/QUD.png){: .img-width-half }
- QUDs는 전반적으로  훨씬 중요함

## Salience Prediction

### 질문 유효성 분류기

Invalid 질문 비율 9.1%  → 대부분 앵커 이슈가 원인 → 중요성 점수 매기기 전 invalid 질문 제외

### Models

- Instruction Tuning with QLoRA, AdamW
    - input: $C_p, S_k, Q$
    - output: 중요성 점수
    - 모델
        - Mistral-7B-Instruct
        - Llama-2-7b-chat
        - TinyLlama-1.1B-chat
        - Flan-t5-base: context
- LLM Zero-/Few-shot Baselines(with GPT-4-turbo)
    - Zero-shot (vanilla)
    - Few-shot (vanilla)
        - 15개의 in-context 학습 예시(레이블 당 3개)
        - LLMd의 최근 편향을 활용해 레이블 분포와 더 잘 일치하도록 예측 유도
            - 마지막 예시가 더 빈번한 레이블을 갖도록 in-context demonstrations 순서 바꿈
    - Few-shot (kNN)
        - LLM의 성능은 in-context 예시에 민감함 → kNN 기반 방식 사용(유클리드 거리)
    - Chain-of-Thoug(CoT)
        - few-shot CoT: in-context 예시 5개 + 주석가가 단 중요성 점수에 대한 근거

### Evaluation

- Data
    - Training: 1,228 / Validation: 143 / Test: 235
    - 균등한 레이블 분포를 위해 훈련 데이터만 업샘플링 → 2355(각 레이블 당 471개 예시)
- Evaluation Metrics
    - 예측 점수와 인간 점수 간의 MAE(Mean Absolute Error)
    - 예측 점수와 인간 점수 간의 Spearman’s ρ
    - macro-averaged F1
    - Krippendorff’s α → 주석가 간 Agreement
- Results <br>
    ![Result]({{site.url}}/assets/images/Inquisitive Questions/Result.png){: .img-width-half }
    - fine-tuning 모델이 zero/few shot LLM보다 성능 좋음
    - fine-tuning 소형 모델 중에서 가장 성능이 좋은 Mistral 기반 모델이 Krippendorff’s α 제일 높음
    - 매개변수가 250B에 불과하고 context window가 작은 Flan-t5-base도 fine-tuning 하면 경쟁력 있음
    
        → LLM 프롬프팅이나 in-context 학습에서는 질문의 중요성을 끌어내기 어려움

        → 명시적인 훈련(fine-tuning)이 더 효과적
    
- Confusion Matrix<br>
    ![ConfusionMatrix]({{site.url}}/assets/images/Inquisitive Questions/Conf_Matrix.png){: .img-width-half }
    - GPT-4는 4와 5를 많이 예측함 → 좋은 질문과 나쁜 질문을 구분할 수 없음
    - Fine-tuning 모델은 서로 더 가까운 레이블을 혼동하는 경향이 있음 → GPT-4를 사용한 in-context 학습보다 해당 작업을 더 잘 이해함

## Use case: Do better expanded summaries answer more salient question?

![Expanded]({{site.url}}/assets/images/Inquisitive Questions/Expanded.png)
- TL;DR을 읽은 후 독자가 전체 기사를 읽을지 결정하기 위해 더 많은 것을 알고 싶어하는 상황
- 더 좋은 요약이 더 중요한 질문에 답변하는가?

### Data

- Articles and TL;DRs: GPT-4-turbo를 사용해 기사에 대한 TL;DR(50 글자 요약) 생성
- Expanded Summaries: 각 TL;DR에서 250-300 단어 분량의 확장 요약 3개 생성
    - GPT-4
        - $(D, S_s)$를 주고, GPT-4-turbo에게 $S_l$ 생성 요청
    - Flan-T5
        - 정교함을 위해 Flan-T5-large 사용
        - 문장 반복과 길이 조절 위반 등의 문제 발생 → 결과 수정하는 동시에 GPT-4-turbo로 최대한 요약 보존
    - GPT-4-Corrupted
        - 주제가 누락된 기준 역할을 하는 “Bad” 요약 생성
        - GPT-4-turbo에게 $(D, S_s)$를 입력으로 주고 $S_s$의 주제를 찾으라고 함
        - 주제를 포함하지 않는 긴 요약을 생성하라고 함
            - 일관성 및 길이 제어 위반 발생 가능 → Flan-T5와 동일한 조치 수행
- Question Salience
    - 모든 TL;DR에서 각 문장은 LLM이 생성한 중요성 주석이 달린 5개의 inquisitive question과 연관됨
    - Fine-tuning 모델로 중요성 수치 예측

### Evaluation

- Human Ranking of Summaries
    - 5명의 주석가에게 내용 위주로 확장된 요약의 순위 매기게 하고 순위에 해당하는 점수 부여(1~3)
    - 순위 결과는 예상되는 품질 순서를 보여줌
        - GPT-4 (2.91) → Flan-T5 (1.73) → GPT-4-Corrupted (1.35)
        - 확장 요약에서 QUD의 중요성을 활용하여 재현하고자 하는 **Oralce**
- Measuring QUD Salience
    - QUDs가 되는 각 TL;DR S_s에서 유발되는 질문들의 중요성 점수(src) 측정
        - 기사 D에서 답변되지 않는 질문 필터링
        - GPT-4-turbo로 남은 질문과 $S_l$(Summary)을 사용해 $S_l$이 답변하는 모든 질문 리턴
        - $SummScr = \frac{1}{n} \sum_{i=1}^{n}\sum_{q \in Q_i}scr(q)$ //$Q_i$는 확장 요약 i의 모든 답변된 질문
- Results<br>
    ![Results]({{site.url}}/assets/images/Inquisitive Questions/Results.png){: .img-width-half }
    - 다수 요약 순위와 SummScr(human) 사이의 Kendall’s τ 상관 관계: 0.529
    - SummScr
        - GPT-4 (21.93) → Flan-T5 (16.25) → GPT-4-Corrupted (8.81)
        
        ⇒  Oracle과 일치함 ⇒ **더 나은 요약이 더 중요한 질문을 답변함**
        
## Limitations

- potential question의 의미에 대해 고려 X
- 정보의 유용성 측정 X
- Only English Text
- 작가의 예상과 독자의 배경지식 간의 수준 차이가 있다면 사용 불가

# 소감

- 기존의 질문 생성이나 답변 시스템이 주로 "정확한 답변 생성"에 초점을 맞췄다면, 이 논문은 질문 자체의 중요성을 모델링한다는 점에서 새로운 관점을 제시
- 프롬프트 설계만으로는 중요한 질문을 예측하기 어렵다는 결과는 커스텀 모델 설계와 도메인 특화 데이터셋의 중요성을 보여주는 사례
- 더 나은 요약이 더 많은 중요한 질문에 답변한다는 새로운 뉴스 요약 성능 지표를 제함